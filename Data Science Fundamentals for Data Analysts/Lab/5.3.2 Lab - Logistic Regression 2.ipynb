{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"398e7ab8-fc2d-4841-8260-0322d2a47a1a"}}},{"cell_type":"markdown","source":["# Logistic Regression Lab 2\n\n**Objectives**:\n1. Perform a train-test split on data.\n1. Evaluate four multi-variable logistic regression models using accuracy\nand a confusion matrix.\n\nAdditionally, you will be asked to consider overfitting and underfitting\nof the models based upon these results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d52b0b27-196c-4a34-9824-1b46220ab707"}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6918e0fd-74d2-4413-9fe6-075b8f4e6440"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Mounting course-specific datasets to <b>/mnt/training</b>...</br>Datasets are already mounted to <b>/mnt/training</b> from <b>s3a://databricks-corp-training/common</b>","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Mounting course-specific datasets to <b>/mnt/training</b>...</br>Datasets are already mounted to <b>/mnt/training</b> from <b>s3a://databricks-corp-training/common</b>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: DataFrame[]</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res1: Boolean = false\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res1: Boolean = false\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res2: Boolean = false\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: Boolean = false\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Setup\n\n### Load the Data\nThe `Includes/Classroom-Setup` notebook has made an aggregate table of data\navailable to us via the Metastore associated with our workspace. We can load\nthe data as a pandas dataframe using the cell below.\n\nThis command loads the table using the Metastore reference. The `.toPandas()`\nmethod converts the Spark DataFrame to a Pandas DataFrame. We will use the\nPandas DataFrame with Scikit-Learn throughout this Module."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"991a031a-e24c-45d3-98ef-1c1c7f7b1458"}}},{"cell_type":"code","source":["ht_agg_spark_df = spark.read.table(\"ht_agg\")\nht_agg_pandas_df = ht_agg_spark_df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08ed8eae-cc59-4f95-a7b2-d67584a552c3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Prepare Four Datasets and Target\n\nNext, we will prepare four subsets of our, used as in the previous lab\nto build four different logistic regression models.\n\nWe also prepare our target vector, `y`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61e1b15b-7c37-4350-91a3-4ca7e70f6a80"}}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n\nX_1 = ht_agg_pandas_df[['mean_active_heartrate', 'mean_resting_heartrate']]\nX_2 = ht_agg_pandas_df[['mean_active_heartrate', 'mean_vo2']]\nX_3 = ht_agg_pandas_df[['mean_active_heartrate', 'mean_bmi', 'mean_vo2']]\nX_4 = ht_agg_pandas_df[['mean_active_heartrate', 'mean_bmi', 'mean_vo2', 'mean_resting_heartrate']]\n\nle = LabelEncoder()\nlifestyle = ht_agg_pandas_df['lifestyle']\nle.fit(lifestyle)\ny = le.transform(lifestyle)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62be284e-8b23-49f5-af9c-cc0157ea7e52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Framing a Business Problem\n\nOver the next few labs, we will use supervised machine learning\nto answer a new business question:\n\n> Given a users fitness profile, can we predict the lifestyle of a user?\n\nLike the regression problem we previously solved,\nour **inputs** will be fitness profile information. This is, however, a classification\nproblem and will have a different **output**, lifestyle."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75e8b4aa-6fc4-4955-aed9-c734915079fd"}}},{"cell_type":"markdown","source":["### Perform the Train-Test Split\n\nNext, we will split one of our four subsets of feature data and our target data\ninto training and testing data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a27f0129-b359-4f98-ad15-f827912afef7"}}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n\nX_1_train, X_1_test, y_train, y_test = train_test_split(X_1, y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7465e999-e1e0-46b9-b5f5-06b6e3569928"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Your Turn\n\n### Exercise 1: Perform the Train-Test Split\n\nPerform the train-test split on the remaining data subsets:\n1. use the helper function `train_test_split`\n1. split the following subsets:\n   - `X_2`, `X_3`, `X_4`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fb8435b-74e8-48e6-bbbf-8082c9ac6115"}}},{"cell_type":"code","source":["# TODO\nX_2_train, X_2_test, y_train, y_test = train_test_split(X_2, y)\nX_3_train, X_3_test, y_train, y_test = train_test_split(X_3, y)\nX_4_train, X_4_test, y_train, y_test = train_test_split(X_4, y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74239588-a0b3-438c-8eb5-ac7331fcaebb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Exercise 2: Multi-Variable Logistic Regression\n\nFit four multiple-variable logistic models, one for each datasubset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1b3830c-2877-4ec3-ab04-4c65cea8d451"}}},{"cell_type":"code","source":["# TODO\nfrom sklearn.linear_model import LogisticRegression\nlr_1 = LogisticRegression(max_iter=10000)\nlr_2 = LogisticRegression(max_iter=10000)\nlr_3 = LogisticRegression(max_iter=10000)\nlr_4 = LogisticRegression(max_iter=10000)\n\nlr_1.fit(X_1_train, y_train)\nlr_2.fit(X_2_train, y_train)\nlr_3.fit(X_3_train, y_train)\nlr_4.fit(X_4_train, y_train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"737de8f4-c3b2-4cf8-a98e-5a0c652cfdab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[18]: LogisticRegression(max_iter=10000)</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: LogisticRegression(max_iter=10000)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Demonstration\n### Evaluate a Multi-variable Model using accuracy and a confusion matrix\n\nFinally, we evaulate our models. We do so using the accuracy metric and a confusion matrix.\n\nTo use these metrics, we need to\n1. generate a vector of precictions using `estimator.predict()`\n1. pass actual and predicted values to the metric as `metric(actual, predicted)`\n1. do this for both the training and testing data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a16fbb80-2ddf-4fc5-be14-d1fec4d0bd45"}}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, confusion_matrix\n\ny_train_1_predicted = lr_1.predict(X_1_train)\ny_test_1_predicted = lr_1.predict(X_1_test)\n\nprint(\"training accuracy: \", accuracy_score(y_train, y_train_1_predicted))\nprint(\"test accuracy:     \", accuracy_score(y_test, y_test_1_predicted))\nprint(\"training confusion matrix\")\nprint(confusion_matrix(y_train, y_train_1_predicted))\nprint(\"\")\nprint(\"test confusion matrix\")\nprint(confusion_matrix(y_test, y_test_1_predicted))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5d562db-c6a6-48f8-9489-039edb9ae6c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">training accuracy:  0.35644444444444445\ntest accuracy:      0.34933333333333333\ntraining confusion matrix\n[[  0 643   0   0]\n [  0 802   0   0]\n [  0 233   0   0]\n [  1 571   0   0]]\n\ntest confusion matrix\n[[  0 216   0   0]\n [  0 262   0   0]\n [  0  79   0   0]\n [  0 193   0   0]]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">training accuracy:  0.35644444444444445\ntest accuracy:      0.34933333333333333\ntraining confusion matrix\n[[  0 643   0   0]\n [  0 802   0   0]\n [  0 233   0   0]\n [  1 571   0   0]]\n\ntest confusion matrix\n[[  0 216   0   0]\n [  0 262   0   0]\n [  0  79   0   0]\n [  0 193   0   0]]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question**: What do you notice about the results?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4b886e3-6e4f-4f36-b543-456571564f2a"}}},{"cell_type":"markdown","source":["## Your Turn\n### Exercise 3: Generate Predictions\n1. use the following subset splits:\n   - `X_1_test`, `X_2_test`, `X_3_test`, `X_4_test`\n   - `X_1_train`, `X_2_train`, `X_3_train`, `X_4_train`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"464391e2-cc02-447c-940a-08a13a5a2c67"}}},{"cell_type":"code","source":["# TODO\ny_train_1_predicted = lr_1.predict(X_1_train)\ny_test_1_predicted = lr_1.predict(X_1_test)\ny_train_2_predicted = lr_2.predict(X_2_train)\ny_test_2_predicted = lr_2.predict(X_2_test)\ny_train_3_predicted = lr_3.predict(X_3_train)\ny_test_3_predicted = lr_3.predict(X_3_test)\ny_train_4_predicted = lr_4.predict(X_4_train)\ny_test_4_predicted = lr_4.predict(X_4_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bccac99a-9144-41a7-899d-c7a18b17b42b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Exercise 4: Evaluate Our Models\n\n1. Use the `accuracy_score` and `confusion_matrix` metrics\n1. don't forget to take the square root of the mean squared error\n1. use the following subset splits:\n   - `X_2_test`, `X_3_test`, `X_4_test`\n   - `X_2_train`, `X_3_train`, `X_4_train`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdf2bda3-2e8d-4aa3-ab79-4496604a492a"}}},{"cell_type":"code","source":["# TODO\ntrain_1_accuracy = accuracy_score(y_train, y_train_1_predicted)\ntrain_1_conf_mat = confusion_matrix(y_train, y_train_1_predicted)\ntest_1_accuracy = accuracy_score(y_test, y_test_1_predicted)\ntest_1_conf_mat = confusion_matrix(y_test, y_test_1_predicted)\n\ntrain_2_accuracy = accuracy_score(y_train, y_train_2_predicted)\ntrain_2_conf_mat = confusion_matrix(y_train, y_train_2_predicted)\ntest_2_accuracy = accuracy_score(y_test, y_test_2_predicted)\ntest_2_conf_mat = confusion_matrix(y_test, y_test_2_predicted)\n\ntrain_3_accuracy = accuracy_score(y_train, y_train_3_predicted)\ntrain_3_conf_mat = confusion_matrix(y_train, y_train_3_predicted)\ntest_3_accuracy = accuracy_score(y_test, y_test_3_predicted)\ntest_3_conf_mat = confusion_matrix(y_test, y_test_3_predicted)\n\ntrain_4_accuracy = accuracy_score(y_train, y_train_4_predicted)\ntrain_4_conf_mat = confusion_matrix(y_train, y_train_4_predicted)\ntest_4_accuracy = accuracy_score(y_test, y_test_4_predicted)\ntest_4_conf_mat = confusion_matrix(y_test, y_test_4_predicted)\n\nprint(\"model 1: training accuracy: \", train_1_accuracy)\nprint(\"model 1: training confusion matrix: \")\nprint(train_1_conf_mat)\nprint(\" \")\nprint(\"model 1: test accuracy:     \", test_1_accuracy)\nprint(\"model 1: test confusion matrix:     \")\nprint(test_1_conf_mat)\nprint(\" \")\nprint(\"model 2: training accuracy: \", train_2_accuracy)\nprint(\"model 2: training confusion matrix: \")\nprint(train_2_conf_mat)\nprint(\" \")\nprint(\"model 2: test accuracy:     \", test_2_accuracy)\nprint(\"model 2: test confusion matrix:     \")\nprint(test_2_conf_mat)\nprint(\" \")\nprint(\"model 3: training accuracy: \", train_3_accuracy)\nprint(\"model 3: training confusion matrix: \")\nprint(train_3_conf_mat)\nprint(\" \")\nprint(\"model 3: test accuracy:     \", test_3_accuracy)\nprint(\"model 3: test confusion matrix:     \")\nprint(test_3_conf_mat)\nprint(\" \")\nprint(\"model 4: training accuracy: \", train_4_accuracy)\nprint(\"model 4: training confusion matrix: \")\nprint(train_4_conf_mat)\nprint(\" \")\nprint(\"model 4: test accuracy:     \", test_4_accuracy)\nprint(\"model 4: test confusion matrix:     \")\nprint(test_4_conf_mat)\nprint(\" \")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00c3aff7-a765-43aa-a70c-bcc2d43293bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">model 1: training accuracy:  0.35644444444444445\nmodel 1: training confusion matrix: \n[[  0 643   0   0]\n [  0 802   0   0]\n [  0 233   0   0]\n [  1 571   0   0]]\n \nmodel 1: test accuracy:      0.34933333333333333\nmodel 1: test confusion matrix:     \n[[  0 216   0   0]\n [  0 262   0   0]\n [  0  79   0   0]\n [  0 193   0   0]]\n \nmodel 2: training accuracy:  0.35555555555555557\nmodel 2: training confusion matrix: \n[[ 17 626   0   0]\n [ 19 783   0   0]\n [  7 226   0   0]\n [ 13 559   0   0]]\n \nmodel 2: test accuracy:      0.348\nmodel 2: test confusion matrix:     \n[[  6 210   0   0]\n [  7 255   0   0]\n [  2  77   0   0]\n [  5 188   0   0]]\n \nmodel 3: training accuracy:  0.35644444444444445\nmodel 3: training confusion matrix: \n[[  0 643   0   0]\n [  0 802   0   0]\n [  0 233   0   0]\n [  0 572   0   0]]\n \nmodel 3: test accuracy:      0.34933333333333333\nmodel 3: test confusion matrix:     \n[[  0 216   0   0]\n [  0 262   0   0]\n [  0  79   0   0]\n [  0 193   0   0]]\n \nmodel 4: training accuracy:  0.5977777777777777\nmodel 4: training confusion matrix: \n[[312 239   0  92]\n [178 624   0   0]\n [  1   0  61 171]\n [157   7  60 348]]\n \nmodel 4: test accuracy:      0.6093333333333333\nmodel 4: test confusion matrix:     \n[[ 99  78   0  39]\n [ 50 212   0   0]\n [  1   0  25  53]\n [ 43   2  27 121]]\n \n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">model 1: training accuracy:  0.35644444444444445\nmodel 1: training confusion matrix: \n[[  0 643   0   0]\n [  0 802   0   0]\n [  0 233   0   0]\n [  1 571   0   0]]\n \nmodel 1: test accuracy:      0.34933333333333333\nmodel 1: test confusion matrix:     \n[[  0 216   0   0]\n [  0 262   0   0]\n [  0  79   0   0]\n [  0 193   0   0]]\n \nmodel 2: training accuracy:  0.35555555555555557\nmodel 2: training confusion matrix: \n[[ 17 626   0   0]\n [ 19 783   0   0]\n [  7 226   0   0]\n [ 13 559   0   0]]\n \nmodel 2: test accuracy:      0.348\nmodel 2: test confusion matrix:     \n[[  6 210   0   0]\n [  7 255   0   0]\n [  2  77   0   0]\n [  5 188   0   0]]\n \nmodel 3: training accuracy:  0.35644444444444445\nmodel 3: training confusion matrix: \n[[  0 643   0   0]\n [  0 802   0   0]\n [  0 233   0   0]\n [  0 572   0   0]]\n \nmodel 3: test accuracy:      0.34933333333333333\nmodel 3: test confusion matrix:     \n[[  0 216   0   0]\n [  0 262   0   0]\n [  0  79   0   0]\n [  0 193   0   0]]\n \nmodel 4: training accuracy:  0.5977777777777777\nmodel 4: training confusion matrix: \n[[312 239   0  92]\n [178 624   0   0]\n [  1   0  61 171]\n [157   7  60 348]]\n \nmodel 4: test accuracy:      0.6093333333333333\nmodel 4: test confusion matrix:     \n[[ 99  78   0  39]\n [ 50 212   0   0]\n [  1   0  25  53]\n [ 43   2  27 121]]\n \n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question**: Which of these models is the best at predicting lifestyle?\n\n**Question**: Do any of the models show signs of overfitting?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"795f6e26-eab7-4220-a5b7-529693969aec"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71fcd691-4352-495e-a40d-2f30bca947f6"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"5.3.2 Lab - Logistic Regression 2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3673044121708840}},"nbformat":4,"nbformat_minor":0}
